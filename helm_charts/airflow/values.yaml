# Parameters for the git-sync deployment. git-sync pulls code from a repo and saves it in the PVC defined in the dags-pvc.yaml.
# That PVC stores data in Azure File Share and can be then mounted into other pods to make the code available in them. 
#   - repoURL - URL of the repo with dags code to pull (https://github.com/<org-name>/<repo-name>.git)
#   - repoName - Name of the repo (<repo-name>.git)
#   - branch - Branch with code to use
#   - clonePath - Where to clone code in the git-sync pod (which will be mounted into the PVC)
#   - storageAccountName - Name of the Azure Storage Account where to save dags code
#   - fileShareName - Name of the File Share within the Azure Storage Account where to save dags code
#   - secretName - Name of the secret for accessing Storage Account with File Share. It must contains keys:
#       - azurestorageaccountname - Storage Account name
#       - azurestorageaccountkey - Access key
gitSync:
  repoURL: https://github.com/bulka4/data_and_ml_platform.git
  repoName: data_and_ml_platform.git
  dagsFolderPath: apps/airflow/dags
  branch: main
  clonePath: /opt/airflow/dags
  storageAccountName: systemfilesbulka
  fileShareName: airflow-dags
  secretName: airflow-azure-blob

# Secret for pulling images from ACR
# acr:
#   secretName: acr-secret

airflow:
  executor: CeleryExecutor

  images:
    airflow:
      repository: airflow
      tag: latest
  # Secret for pulling images from ACR
  # registry:
  #   secretName: acr-secret
  env:
    - name: AIRFLOW__CORE__LOAD_EXAMPLES
      value: "false"
    - name: AIRFLOW__KUBERNETES_EXECUTOR__NAMESPACE
      value: airflow
    # Saving logs to Azure Storage Account
    # - name: AIRFLOW__LOGGING__REMOTE_LOGGING
    #   value: "True"
    # - name: AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER
    #   value: "wasb://airflow-logs@systemfilesbulka.blob.core.windows.net"
    # - name: AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID
    #   value: "azure_blob" # ID of the connection used for accessing Azure Storage Account for saving Airflow logs
    # Use this service accounts for all pods running Airflow tasks
    - name: AIRFLOW__KUBERNETES__SERVICE_ACCOUNT_NAME
      value: airflow-sa
    # We need to provide webserver URL to see logs in UI. Looks like this URL is sent to a browser and browser uses it
    # to get data about logs.
    # - name: AIRFLOW__WEBSERVER__BASE_URL
    #   value: "http://airflow-webserver"

  # Add PVC with code pulled by the git-sync to the scheduler, workers, triggerer and webserver
  scheduler:
    extraVolumes:
      - name: dags
        persistentVolumeClaim:
          claimName: airflow-dags-pvc # Volume with dags pulled by git-sync (running in a separate pod) and saved in that PV
    extraVolumeMounts:
      - name: dags
        mountPath: /opt/airflow/dags
        #subPath: dags
    # Use our own service account which has specified a secret for pulling images.
    serviceAccount:
      create: False
      name: airflow-sa

  workers:
    # Limit the size of storage for logs
    persistence:
      size: 1Gi
    extraVolumes:
      # Volume with dags pulled by git-sync (running in a separate pod) and saved in that PV.
      # It will be used by the copier container
      - name: dags
        persistentVolumeClaim:
          claimName: airflow-dags-pvc
    extraVolumeMounts:
      - name: dags
        mountPath: /opt/airflow/dags
    # Use our own service account which has specified a secret for pulling images.
    serviceAccount:
      create: False
      name: airflow-sa
    

  triggerer:
    # Limit the size of storage for logs
    persistence:
      size: 1Gi
    extraVolumes:
      - name: dags
        persistentVolumeClaim:
          claimName: airflow-dags-pvc # Volume with dags pulled by git-sync (running in a separate pod) and saved in that PV
    extraVolumeMounts:
      - name: dags
        mountPath: /opt/airflow/dags
        #subPath: dags
    # Use our own service account which has specified a secret for pulling images.
    serviceAccount:
      create: False
      name: airflow-sa

  # Set to false in order not to create a PostgreSQL for Airflow metadata (we create it separately in postgresql.yaml)
  postgresql:
    enabled: false

  # Use the below configuration if we use an external PostgreSQL db, not the one created in this chart by setting
  # postgresql.enabled = true in this file.
  data:
    # Use the secret with the 'connection' key with the connection string to Postgres which will be used by Airflow to connect.
    metadataSecretName: airflow-postgres-connection

  webserver:
    extraVolumes:
      - name: dags
        persistentVolumeClaim:
          claimName: airflow-dags-pvc # Volume with dags pulled by git-sync (running in a separate pod) and saved in that PV
    extraVolumeMounts:
      - name: dags
        mountPath: /opt/airflow/dags
        #subPath: dags
    # Use our own service account which has specified a secret for pulling images.
    serviceAccount:
      create: False
      name: airflow-sa
    # Use the NodePort as a service for the web server, so we can access it through the browser from the host. We also need to map the nodePort
    # to the host port in the kind-config.yaml file.
    # So traffic will go like that: 
    #   - host port (port on the host specified in the kind-config.yaml) -> 
    #   - container port / node port (port in the kind node container, specified in the kind-config.yaml as containerPort and here as nodePort) ->
    #   - target port (port in the container running in the kind cluster, specified here)
    service:
      type: NodePort
      ports:
        - port: 80          # service port (used inside the cluster)
          targetPort: 8080  # container port
          nodePort: 30080   # node port
    # Add this setting to avoid the 'Startup probe failed' error in the api-server pod. We still get that warning even with those settings
    # but everything works now so I will keep those settings just in case (maybe they can be removed later).
    startupProbe:
      timeoutSeconds: 60
      periodSeconds: 10
      failureThreshold: 20
    # Similar as for the startupProbe
    livenessProbe:
      timeoutSeconds: 60
      periodSeconds: 10
      failureThreshold: 20
    # Similar as for the startupProbe
    readinessProbe:
      timeoutSeconds: 60
      periodSeconds: 10
      failureThreshold: 20
    # Increase resource limits and requests so processes in the api-server pod don't die (we can check in the api-server pod logs if they die)
    resources:
      # Requests will be automatically set up to the same value as the limits
      limits:
        cpu: "2000m"
        memory: "2Gi"
    # Run additional init containers to execute commands for additional setup:
    #   - Create a user for accessing UI
    #   - Create a connection to Azure Storage Account for saving logs there
    extraInitContainers:
      # Createe an Airflow user for accessing UI. For some reason Helm parameters for creating the default user doesn't work
      - name: create-admin-user
        image: airflow:latest
        imagePullPolicy: IfNotPresent
        # Create a new user. Use '|| true' (|| is the logical operator OR) so that command doesn't fail when user already exists.
        command:
          - bash
          - -c
          - |
            airflow users create \
              --username admin \
              --firstname Admin \
              --lastname User \
              --role Admin \
              --email admin@example.com \
              --password admin || true
        env:
          # Add env var with a connection string to the PostgreSQL metadata db we use, so Airflow knows which one to use and can connect.
          # Data about the new user will be saved there.
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                name: airflow-postgres-connection
                key: connection