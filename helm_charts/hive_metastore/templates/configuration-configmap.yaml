# Configuration files determining how Hive works, for example which metadata db to use, where the data is stored.

# In pods running the Hive Metastore, in the `/opt/hive/conf` folder we need to have configuration files:
# - hive-site.xml
# - core-site.xml

# We create this config files using a ConfigMap and we will mount them into the pods running the Hive Metastore.

# Every value `${VAR_NAME}` will be replaced with the value of the environment variable `VAR_NAME` using envsubst. We will set up those env vars
# in pods where that ConfigMap will be mounted:
# - DB_PASSWORD is a password to PostgreSQL metadata db
# - CONTAINER - Name of the container in Storage Account where data warehouse data will be stored
# - STORAGE_ACCOUNT - Name of the Storage Account where data warehouse data will be stored
# - SA_ACCESS_KEY - Access key to the Storage Account where data warehouse data will be stored

apiVersion: v1
kind: ConfigMap
metadata:
  name: hive-conf-template
  namespace: {{ .Values.namespace }}
data:
  hive-site.xml.template: |
    <configuration>

      <!-- URL for conneting to the PostgreSQL metadata db -->
      <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>
          jdbc:postgresql://hive-postgres:5432/{{ .Values.hive.database.name }}
        </value>
      </property>

      <!-- Driver to use for conneting to the PostgreSQL metadata db -->
      <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.postgresql.Driver</value>
      </property>

      <!-- Username to use for conneting to the PostgreSQL metadata db -->
      <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>{{ .Values.hive.database.user }}</value>
      </property>

      <!-- Password to use for conneting to the PostgreSQL metadata db -->
      <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>${DB_PASSWORD}</value>
      </property>

      <!-- URL to use for conneting to the Hive Metastore (used here only for using hive CLI in the pod running Metastore for testing) -->
      <property>
        <name>hive.metastore.uris</name>
        <value>
          thrift://localhost:{{ .Values.hive.metastorePort }}
        </value>
      </property>

      <!-- This config must be the same as the spark.sql.warehouse.dir config in Spark's server, spark-defaults.conf file -->
      <!-- Specify where the data will be stored -->
      <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>{{ .Values.hive.warehouseDir }}</value>
      </property>

      <property>
          <name>hive.metastore.client.factory.class</name>
          <value>org.apache.hadoop.hive.metastore.HiveMetaStoreClientFactory</value>
      </property>
      
      <property>
          <name>hive.exec.scratchdir</name>
          <value>/tmp/hive</value>
      </property>

      <property>
        <name>datanucleus.autoCreateSchema</name>
        <value>true</value>
      </property>

      <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
      </property>

      <!-- Block certain changes of column types (like from int to string) -->
      <property>
        <name>hive.metastore.disallow.incompatible.col.type.changes</name>
        <value>false</value>
      </property>

      <!-- Dont use Kerberos authentication. This config must be the same at the client side (e.g. Spark) -->
      <property>
        <name>hive.metastore.sasl.enabled</name>
        <value>false</value>
      </property>

      <!-- Allow to automatically create partitions -->
      <property>
        <name>hive.exec.dynamic.partition</name>
        <value>true</value>
      </property>

      <!-- All partition columns can be dynamic. Fully automatic partition creation. -->
      <property>
        <name>hive.exec.dynamic.partition.mode</name>
        <value>nonstrict</value>
      </property>

    </configuration>

  core-site.xml.template: |
    <configuration>
        <!-- Default filesystem URI. Used when writing / reading data -->
        <property>
            <name>fs.defaultFS</name>
            <value>abfss://${CONTAINER}@${STORAGE_ACCOUNT}.dfs.core.windows.net/</value>
        </property>

        <!-- Access key for authentication to Azure Storage Account. STORAGE_ACCOUNT is a name of the Storage Account -->
        <!-- to use and SA_ACCESS_KEY is an access key to it. -->
        <property>
          <name>fs.azure.account.key.${STORAGE_ACCOUNT}.dfs.core.windows.net</name>
          <value>${SA_ACCESS_KEY}</value>
        </property>
    </configuration>