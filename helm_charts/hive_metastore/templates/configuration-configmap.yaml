# In pods running the Hive Metastore, in the `/opt/hive/conf` folder we need to have configuration files:
# - `hive-site.xml`

# We create this config files using a ConfigMap and we will mount them into the pods running the Hive Metastore.

# Every value `${VAR_NAME}` will be replaced with the value of the environment variable `VAR_NAME` which we will set up in pods where that ConfigMap 
# will be mounted (it will be done by Hive inside the image, not Kubernetes).

# - ${DB_PASSWORD} is a password to PostgreSQL metadata db

apiVersion: v1
kind: ConfigMap
metadata:
  name: hive-conf-template
  namespace: {{ .Values.namespace }}
data:
  hive-site.xml.template: |
    <configuration>

      <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>
          jdbc:postgresql://hive-postgres:5432/{{ .Values.hive.database.name }}
        </value>
      </property>

      <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.postgresql.Driver</value>
      </property>

      <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>{{ .Values.hive.database.user }}</value>
      </property>

      <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>${DB_PASSWORD}</value>
      </property>

      <property>
        <name>hive.metastore.uris</name>
        <value>
          thrift://localhost:{{ .Values.hive.metastorePort }}
        </value>
      </property>

      <!-- This config must be the same as the spark.sql.warehouse.dir config in Spark's server, spark-defaults.conf file -->
      <!-- Specify where the data will be stored -->
      <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>{{ .Values.hive.warehouseDir }}</value>
      </property>

      <property>
          <name>hive.metastore.client.factory.class</name>
          <value>org.apache.hadoop.hive.metastore.HiveMetaStoreClientFactory</value>
      </property>
      
      <property>
          <name>hive.exec.scratchdir</name>
          <value>/tmp/hive</value>
      </property>

      <property>
        <name>datanucleus.autoCreateSchema</name>
        <value>true</value>
      </property>

      <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
      </property>

      <!-- Block certain changes of column types (like from int to string) -->
      <property>
        <name>hive.metastore.disallow.incompatible.col.type.changes</name>
        <value>false</value>
      </property>

      <!-- Dont use Kerberos authentication. This config must be the same at the client side (e.g. Spark) -->
      <property>
        <name>hive.metastore.sasl.enabled</name>
        <value>false</value>
      </property>

      <!-- Allow to automatically create partitions -->
      <property>
        <name>hive.exec.dynamic.partition</name>
        <value>true</value>
      </property>

      <!-- All partition columns can be dynamic. Fully automatic partition creation. -->
      <property>
        <name>hive.exec.dynamic.partition.mode</name>
        <value>nonstrict</value>
      </property>

    </configuration>
  
  core-site.xml.template: |
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>abfss://${CONTAINER}@${STORAGE_ACCOUNT}.dfs.core.windows.net/</value>
        </property>

        <property>
          <name>fs.azure.account.key.<STORAGE_ACCOUNT>.dfs.core.windows.net</name>
          <value>${SA_ACCESS_KEY}</value>
        </property>

        <!--
          <property>
              <name>fs.azure.account.auth.type.${STORAGE_ACCOUNT}.dfs.core.windows.net</name>
              <value>OAuth</value>
          </property>
          <property>
              <name>fs.azure.account.oauth.provider.type.${STORAGE_ACCOUNT}.dfs.core.windows.net</name>
              <value>org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider</value>
          </property>
          <property>
              <name>fs.azure.account.oauth2.client.id.${STORAGE_ACCOUNT}.dfs.core.windows.net</name>
              <value>${CLIENT_ID}</value>
          </property>
          <property>
              <name>fs.azure.account.oauth2.client.secret.${STORAGE_ACCOUNT}.dfs.core.windows.net</name>
              <value>${CLIENT_SECRET}</value>
          </property>
          <property>
              <name>fs.azure.account.oauth2.client.endpoint.${STORAGE_ACCOUNT}.dfs.core.windows.net</name>
              <value>https://login.microsoftonline.com/${TENANT_ID}/oauth2/token</value>
          </property>
        -->
    </configuration>